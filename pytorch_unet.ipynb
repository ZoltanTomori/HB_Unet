{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"pytorch_unet.py\"):\n",
    "  if not os.path.exists(\"pytorch_unet\"):\n",
    "    !git clone https://github.com/ZoltanTomori/HB_Unet.git\n",
    "\n",
    "  %cd HB_Unet\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from celldata import CellDataset\n",
    "from augmentation import (\n",
    "    DoubleCompose, DoubleToTensor, GaussianNoise,\n",
    "    DoubleHorizontalFlip, DoubleVerticalFlip, DoubleElasticTransform\n",
    ")\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_train_loader(mean, std, out_size, batch_size, pct=.9):\n",
    "    \"\"\"Initialize Dataloader for training set\n",
    "\n",
    "        mean (float): mean of pixel values\n",
    "        std (float): standard deviation of pixel values\n",
    "        out_size (int): dimension of segmentation map (out_size x out_size)\n",
    "        batch_size (int): number of samples to load for each iteration\n",
    "        pct (float): percentage of data to use for training (0 < pct <= 1)\n",
    "    \"\"\"\n",
    "    image_mask_transform = DoubleCompose([\n",
    "        DoubleToTensor(),\n",
    "        DoubleElasticTransform(alpha=250, sigma=10),\n",
    "        DoubleHorizontalFlip(),\n",
    "        DoubleVerticalFlip()\n",
    "    ])\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.ColorJitter(brightness=0.4),\n",
    "        transforms.Normalize(mean, std),\n",
    "        GaussianNoise(),\n",
    "        transforms.Pad(30, padding_mode='reflect')\n",
    "    ])\n",
    "    mask_transform = transforms.CenterCrop(out_size)\n",
    "\n",
    "    train_data = CellDataset(\n",
    "        image_mask_transform=image_mask_transform,\n",
    "        image_transform=image_transform,\n",
    "        mask_transform=mask_transform,\n",
    "        pct=pct\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "def get_test_loader(mean, std, out_size, batch_size):\n",
    "    \"\"\"Initialize Dataloader for validation set\n",
    "\n",
    "        mean (float): mean of pixel values\n",
    "        std (float): standard deviation of pixel values\n",
    "        out_size (int): dimension of segmentation map (out_size x out_size)\n",
    "        batch_size (int): number of samples to load for each iteration\n",
    "    \"\"\"\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "        transforms.Pad(30, padding_mode='reflect')\n",
    "    ])\n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.CenterCrop(388)\n",
    "    ])\n",
    "\n",
    "    test_data = CellDataset(\n",
    "        image_transform=image_transform,\n",
    "        mask_transform=mask_transform,\n",
    "        data_type='validate'\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\PyProjects\\\\HB_Unet\\\\data\\\\train-volume.tif'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m std \u001b[39m=\u001b[39m \u001b[39m0.173\u001b[39m\n\u001b[0;32m      7\u001b[0m out_size \u001b[39m=\u001b[39m \u001b[39m388\u001b[39m  \u001b[39m# output dimension of segmentation map\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m train_loader \u001b[39m=\u001b[39m get_train_loader(mean, std, out_size, batch_size\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[0;32m      9\u001b[0m test_loader \u001b[39m=\u001b[39m get_test_loader(mean, std, out_size, test_batch_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "Cell \u001b[1;32mIn[51], line 32\u001b[0m, in \u001b[0;36mget_train_loader\u001b[1;34m(mean, std, out_size, batch_size, pct)\u001b[0m\n\u001b[0;32m     24\u001b[0m image_transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m     25\u001b[0m     transforms\u001b[39m.\u001b[39mColorJitter(brightness\u001b[39m=\u001b[39m\u001b[39m0.4\u001b[39m),\n\u001b[0;32m     26\u001b[0m     transforms\u001b[39m.\u001b[39mNormalize(mean, std),\n\u001b[0;32m     27\u001b[0m     GaussianNoise(),\n\u001b[0;32m     28\u001b[0m     transforms\u001b[39m.\u001b[39mPad(\u001b[39m30\u001b[39m, padding_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mreflect\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m ])\n\u001b[0;32m     30\u001b[0m mask_transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCenterCrop(out_size)\n\u001b[1;32m---> 32\u001b[0m train_data \u001b[39m=\u001b[39m CellDataset(\n\u001b[0;32m     33\u001b[0m     image_mask_transform\u001b[39m=\u001b[39;49mimage_mask_transform,\n\u001b[0;32m     34\u001b[0m     image_transform\u001b[39m=\u001b[39;49mimage_transform,\n\u001b[0;32m     35\u001b[0m     mask_transform\u001b[39m=\u001b[39;49mmask_transform,\n\u001b[0;32m     36\u001b[0m     pct\u001b[39m=\u001b[39;49mpct\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     38\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(\n\u001b[0;32m     39\u001b[0m     train_data,\n\u001b[0;32m     40\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m     41\u001b[0m     shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     43\u001b[0m \u001b[39mreturn\u001b[39;00m train_loader\n",
      "File \u001b[1;32md:\\PyProjects\\HB_Unet\\celldata.py:78\u001b[0m, in \u001b[0;36mCellDataset.__init__\u001b[1;34m(self, root_dir, image_mask_transform, image_transform, mask_transform, pct, data_type, in_size, out_size, w0, sigma, weight_map_dir)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose(\n\u001b[0;32m     74\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_transform\u001b[39m.\u001b[39mtransforms[\u001b[39m1\u001b[39m:]\n\u001b[0;32m     75\u001b[0m     )\n\u001b[0;32m     77\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m---> 78\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimages \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39;49mimread(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_path)\n\u001b[0;32m     79\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmasks \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mimread(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_path)\n\u001b[0;32m     80\u001b[0m myImages \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\PyTorch_env\\lib\\site-packages\\skimage\\io\\_io.py:53\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[0;32m     50\u001b[0m         plugin \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtifffile\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     52\u001b[0m \u001b[39mwith\u001b[39;00m file_or_url_context(fname) \u001b[39mas\u001b[39;00m fname:\n\u001b[1;32m---> 53\u001b[0m     img \u001b[39m=\u001b[39m call_plugin(\u001b[39m'\u001b[39m\u001b[39mimread\u001b[39m\u001b[39m'\u001b[39m, fname, plugin\u001b[39m=\u001b[39mplugin, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mplugin_args)\n\u001b[0;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(img, \u001b[39m'\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\PyTorch_env\\lib\\site-packages\\skimage\\io\\manage_plugins.py:207\u001b[0m, in \u001b[0;36mcall_plugin\u001b[1;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCould not find the plugin \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m for \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m    205\u001b[0m                            (plugin, kind))\n\u001b[1;32m--> 207\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\PyTorch_env\\lib\\site-packages\\skimage\\io\\_plugins\\tifffile_plugin.py:30\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, **kwargs)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mimg_num\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m     28\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mkey\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mimg_num\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m \u001b[39mreturn\u001b[39;00m tifffile_imread(fname, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\PyTorch_env\\lib\\site-packages\\tifffile\\tifffile.py:816\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(files, aszarr, **kwargs)\u001b[0m\n\u001b[0;32m    813\u001b[0m     files \u001b[39m=\u001b[39m files[\u001b[39m0\u001b[39m]\n\u001b[0;32m    815\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(files, (\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike)) \u001b[39mor\u001b[39;00m \u001b[39mhasattr\u001b[39m(files, \u001b[39m'\u001b[39m\u001b[39mseek\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 816\u001b[0m     \u001b[39mwith\u001b[39;00m TiffFile(files, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs_file) \u001b[39mas\u001b[39;00m tif:\n\u001b[0;32m    817\u001b[0m         \u001b[39mif\u001b[39;00m aszarr:\n\u001b[0;32m    818\u001b[0m             \u001b[39mreturn\u001b[39;00m tif\u001b[39m.\u001b[39maszarr(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\PyTorch_env\\lib\\site-packages\\tifffile\\tifffile.py:3000\u001b[0m, in \u001b[0;36mTiffFile.__init__\u001b[1;34m(self, arg, mode, name, offset, size, _multifile, _useframes, _master, **kwargs)\u001b[0m\n\u001b[0;32m   2997\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr+b\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m   2998\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39minvalid mode \u001b[39m\u001b[39m{\u001b[39;00mmode\u001b[39m!r}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 3000\u001b[0m fh \u001b[39m=\u001b[39m FileHandle(arg, mode\u001b[39m=\u001b[39;49mmode, name\u001b[39m=\u001b[39;49mname, offset\u001b[39m=\u001b[39;49moffset, size\u001b[39m=\u001b[39;49msize)\n\u001b[0;32m   3001\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fh \u001b[39m=\u001b[39m fh\n\u001b[0;32m   3002\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multifile \u001b[39m=\u001b[39m \u001b[39mbool\u001b[39m(_multifile)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\PyTorch_env\\lib\\site-packages\\tifffile\\tifffile.py:9537\u001b[0m, in \u001b[0;36mFileHandle.__init__\u001b[1;34m(self, file, mode, name, offset, size)\u001b[0m\n\u001b[0;32m   9535\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_file \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   9536\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock \u001b[39m=\u001b[39m NullContext()\n\u001b[1;32m-> 9537\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopen()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\PyTorch_env\\lib\\site-packages\\tifffile\\tifffile.py:9550\u001b[0m, in \u001b[0;36mFileHandle.open\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   9548\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mrealpath(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file)\n\u001b[0;32m   9549\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file)\n\u001b[1;32m-> 9550\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fh \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_file, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mode)\n\u001b[0;32m   9551\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   9552\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_offset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\PyProjects\\\\HB_Unet\\\\data\\\\train-volume.tif'"
     ]
    }
   ],
   "source": [
    "# Nacitanie obrazkov a masiek\n",
    "\n",
    "\n",
    "\n",
    "mean = 0.495\n",
    "std = 0.173\n",
    "out_size = 388  # output dimension of segmentation map\n",
    "train_loader = get_train_loader(mean, std, out_size, batch_size=3)\n",
    "test_loader = get_test_loader(mean, std, out_size, test_batch_size=3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulacia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper\n",
    "import simulation\n",
    "\n",
    "# Generate some random images\n",
    "input_images, target_masks = simulation.generate_random_data(192, 192, count=3)\n",
    "\n",
    "for x in [input_images, target_masks]:\n",
    "    print(x.shape)\n",
    "    print(x.min(), x.max())\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [x.astype(np.uint8) for x in input_images]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in target_masks]\n",
    "\n",
    "# Left: Input image, Right: Target mask (Ground-truth)\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "class SimDataset(Dataset):\n",
    "    def __init__(self, count, transform=None):\n",
    "        self.input_images, self.target_masks = simulation.generate_random_data(192, 192, count=count)        \n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_images)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        image = self.input_images[idx]\n",
    "        mask = self.target_masks[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return [image, mask]\n",
    "\n",
    "# use same transform for train/val for this example\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_set = SimDataset(2000, transform = trans)\n",
    "val_set = SimDataset(200, transform = trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 25\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    x: len(image_datasets[x]) for x in image_datasets.keys()\n",
    "}\n",
    "\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.utils\n",
    "\n",
    "def reverse_transform(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    inp = (inp * 255).astype(np.uint8)\n",
    "    \n",
    "    return inp\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, masks = next(iter(dataloaders['train']))\n",
    "\n",
    "print(inputs.shape, masks.shape)\n",
    "for x in [inputs.numpy(), masks.numpy()]:\n",
    "    print(x.min(), x.max(), x.mean(), x.std())\n",
    "\n",
    "plt.imshow(reverse_transform(inputs[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_unet\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = pytorch_unet.UNet(6)\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from loss import dice_loss\n",
    "\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "        \n",
    "    pred = F.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "    \n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "    \n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):    \n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "        \n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))    \n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "                    \n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)             \n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = calc_loss(outputs, labels, metrics)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = 6\n",
    "\n",
    "model = pytorch_unet.UNet(num_class).to(device)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=25, gamma=0.1)\n",
    "\n",
    "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "\n",
    "import math\n",
    "\n",
    "model.eval()   # Set model to evaluate mode\n",
    "\n",
    "test_dataset = SimDataset(3, transform = trans)\n",
    "test_loader = DataLoader(test_dataset, batch_size=3, shuffle=False, num_workers=0)\n",
    "        \n",
    "inputs, labels = next(iter(test_loader))\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "pred = model(inputs)\n",
    "\n",
    "pred = pred.data.cpu().numpy()\n",
    "print(pred.shape)\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [reverse_transform(x) for x in inputs.cpu()]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in labels.cpu().numpy()]\n",
    "pred_rgb = [helper.masks_to_colorimg(x) for x in pred]\n",
    "\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb, pred_rgb])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "326bd2c82da9b6c6f299e04b27a47450fa1b0607c4ceb31bcfdf7283015a2f86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
